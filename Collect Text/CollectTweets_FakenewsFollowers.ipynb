{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Tweets - Fake sources\n",
    "This notebook collects tweets from users that follow Fake News sources\n",
    "\n",
    "### Reference for recommended manipulations for text analysis:\n",
    "https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "remove_punctuations = False\n",
    "\n",
    "sources_account_list = [\"usa_supreme\",\"rightsidenews\",\"JudicialWatch\",\"100PercentFedUp\",\"ActivistPost\",\"AllenBWest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.toml\", \"rb\") as f:\n",
    "    config = tomli.load(f)\n",
    "    \n",
    "\n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(config[\"twitter_api\"][\"consumer_key\"],\n",
    "                           config[\"twitter_api\"][\"consumer_secret\"])\n",
    "  \n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(config[\"twitter_api\"][\"access_token\"], \n",
    "                      config[\"twitter_api\"][\"access_token_secret\"])\n",
    "  \n",
    "# calling the api \n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect list of twitter IDs of users that follow the fake news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_collect = pd.DataFrame()\n",
    "for user in sources_account_list:\n",
    "    li = api.get_follower_ids(screen_name=user)\n",
    "    data = {\"user_id\": li,\n",
    "        \"because_it_follows\":[user] * len(li) \n",
    "    }\n",
    "    temp_df = pd.DataFrame(data)\n",
    "    user_to_collect = user_to_collect.append(temp_df)\n",
    "    \n",
    "user_to_collect_2 = user_to_collect.groupby(by='user_id').count().reset_index().sort_values(by='because_it_follows', ascending=False)\n",
    "user_to_collect_2 = user_to_collect_2[user_to_collect_2['because_it_follows']>1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each user from the previous list, collect the last 200 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame()\n",
    "for user in tqdm(user_to_collect_2.user_id.unique()):\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            tweets = api.user_timeline(user_id = user, count = 200, tweet_mode=\"extended\", include_rts = False)\n",
    "            for instance in tweets:\n",
    "                tweets_df = tweets_df.append({'user_id':user, 'text':instance.full_text, 'time':instance.created_at}, ignore_index=True)\n",
    "\n",
    "        except tweepy.TweepyException as e:\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process tweet's text for future ML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2752/2752 [00:01<00:00, 1993.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    #Remove hashtags\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Remove URLs\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    #remove punctuations - if needed\n",
    "    if remove_punctuations:\n",
    "        tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    \n",
    "    #to lower\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #translate emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    return tweet\n",
    "    \n",
    "tweets_df['processed_text'] = tweets_df.text.progress_apply(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[['user_id','time','text','processed_text']]\n",
    "\n",
    "#Localize time in order to support saving in Excel\n",
    "tweets_df['date'] = tweets_df['time'].apply(lambda a: pd.to_datetime(a)) \n",
    "tweets_df.to_excel(\"sample_tweets_2.xlsx\", encoding='utf-8')\n",
    "user_to_collect.to_excel(\"users_list_2.xlsx\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parsing (experimental stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "tweets_df = pd.read_excel('sample_tweets_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'four interlocking mega-corporations comprise the corporate media who have vilified alternative opinions as \"conspiracy theories\" and have divided the universe into the simplistic split and projected \"reality\" of juvenal\\'s \"bread and circuses.\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = tweets_df[~tweets_df.processed_text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2409/2409 [00:11<00:00, 204.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def lang_detect(txt):\n",
    "    try:\n",
    "        return detect(txt)\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "\n",
    "\n",
    "tweets_df['language'] = tweets_df.processed_text.progress_apply(lang_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2409/2409 [00:18<00:00, 129.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "\n",
    "# the below function takes text and translates it from arabic to english\n",
    "def translate_arabic( row ):\n",
    "    try:\n",
    "        if row['language'] == 'ar':\n",
    "            translator= Translator(from_lang=\"arabic\",to_lang=\"english\")\n",
    "            return translator.translate(row['processed_text'])\n",
    "        else:\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "tweets_df[\"ar_to_en\"] = tweets_df.progress_apply(translate_arabic, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_excel(\"sample_tweets_2.xlsx\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
